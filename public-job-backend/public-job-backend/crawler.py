# crawler.py

import requests
import os
from bs4 import BeautifulSoup
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from app.database import Base, DATABASE_URL  # DB ì—°ê²° URLê³¼ Baseë¥¼ ì¬ì‚¬ìš©
from app.models import JobPosting        # ê³µê³  ëª¨ë¸ ì„í¬íŠ¸

# ------------------------------------------------------------------
# DB ì—°ê²° ì„¤ì • (FastAPIì™€ ë™ì¼í•œ ì„¤ì • ì‚¬ìš©)
# ------------------------------------------------------------------
# ì£¼ì˜: DATABASE_URLì€ app/database.pyê°€ ë¡œë“œë  ë•Œ ì„¤ì •ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
# (Pythonì´ app.databaseë¥¼ ì½ì„ ë•Œ DATABASE_URLì´ ì •ì˜ë˜ë„ë¡ í”„ë¡œì íŠ¸ êµ¬ì¡°ê°€ ë˜ì–´ ìˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.)

engine = create_engine(DATABASE_URL)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)

def get_db():
    """í¬ë¡¤ëŸ¬ìš© DB ì„¸ì…˜ ìƒì„± í•¨ìˆ˜"""
    db = SessionLocal()
    try:
        yield db
    finally:
        db.close()

def run_crawler():
    db_generator = get_db()
    db = next(db_generator) # DB ì„¸ì…˜ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ê°€ì ¸ì˜´
    
    # ğŸš¨ [ì¤‘ìš”] ì—¬ê¸°ì— ì‹¤ì œ í¬ë¡¤ë§í•  ì›¹ì‚¬ì´íŠ¸ URLì„ ì…ë ¥í•˜ì„¸ìš”.
    URL = "http://example.com" 
    
    try:
        # --- ì˜ˆì‹œ ë°ì´í„° ì¶”ì¶œ (ì‹¤ì œ í¬ë¡¤ë§ ë¡œì§ìœ¼ë¡œ ëŒ€ì²´í•´ì•¼ í•¨) ---
        extracted_data = [
            {
                'company_name': 'AWS EC2 í…ŒìŠ¤íŠ¸ ê¸°ì—…',
                'job_type': 'ì •ê·œì§',
                'title': f'EC2 í™˜ê²½ í¬ë¡¤ë§ í…ŒìŠ¤íŠ¸ ê³µê³  {i}',
                'link': URL + f"/detail/{i}",
                'start_date': '2025-11-12', 
                'end_date': '2025-12-31',
            } for i in range(1, 4) # 3ê°œì˜ í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒì„±
        ]
        # --------------------------------------------------------
        
        # 3. ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥
        new_postings = []
        for data in extracted_data:
            # ì¤‘ë³µ ê²€ì‚¬ ë¡œì§ì„ ì¶”ê°€í•´ì•¼ í•˜ì§€ë§Œ, ì—¬ê¸°ì„œëŠ” ê°„ë‹¨íˆ ì €ì¥í•©ë‹ˆë‹¤.
            new_postings.append(JobPosting(**data))

        db.add_all(new_postings)
        db.commit()
        
        print(f"âœ… SUCCESS: {len(new_postings)}ê°œì˜ ê³µê³  ì •ë³´ë¥¼ DBì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.")

    except requests.exceptions.RequestException as e:
        print(f"âŒ ERROR: HTTP ìš”ì²­ ì˜¤ë¥˜ ë°œìƒ: {e}")
    except Exception as e:
        print(f"âŒ ERROR: ë°ì´í„° ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    finally:
        db_generator.close()

if __name__ == "__main__":
    # DB í…Œì´ë¸”ì´ ì—†ìœ¼ë©´ ìƒì„±
    Base.metadata.create_all(bind=engine)
    
    # í¬ë¡¤ëŸ¬ ì‹¤í–‰
    run_crawler()